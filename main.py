import os
import pandas as pd
from metadata_search import query_faiss_index
from faiss_index import create_faiss_index
from data_search import download_datasets
from llm_chatbot import LLMChatbot
from data_analysis import create_faiss_index_for_data, search_data
import subprocess

def run_webscraping():
    """
    Runs the web scraping script if necessary to generate datasets.csv with metadata.
    """
    print("Running web scraping...")
    subprocess.run(['python', 'webscraping.py'], check=True)

def generate_summary_with_llm(metadata_row, llm_chatbot):
    """
    Generate a new summary for a dataset using the LLM.
    
    Args:
        metadata_row (pd.Series): A row from the dataframe containing title, summary, and links.
        llm_chatbot (LLMChatbot): An instance of the LLMChatbot class.
    
    Returns:
        new_summary (str): A new summary generated by the LLM.
    """
    metadata_content = (
        f"Title: {metadata_row['title']}\n"
        f"Summary: {metadata_row['summary']}\n"
        f"Links: {metadata_row['links']}\n"
    )
    
    prompt = (
        f"Based on the following dataset metadata, generate a detailed summary that describes the dataset:\n\n"
        f"{metadata_content}\n\n"
        "Please provide a clear and concise summary of the dataset."
    )
    
    new_summary = llm_chatbot.generate_response(metadata_content, prompt)
    return new_summary

def generate_summaries_for_datasets(df, llm_chatbot):
    """
    Generate new summaries for all datasets using the LLM and return a dataframe
    with the metadata summaries and links.

    Args:
        df (pd.DataFrame): The dataframe containing dataset metadata (title, summary, links).
        llm_chatbot (LLMChatbot): The LLMChatbot instance used to generate summaries.

    Returns:
        df_with_summaries (pd.DataFrame): The dataframe with the new 'metadatasummary' and 'links' columns.
    """
    df['metadatasummary'] = df.apply(lambda row: generate_summary_with_llm(row, llm_chatbot), axis=1)
    
    # Keep only the metadata summary and links in the final dataset
    return df[['metadatasummary', 'links']]

def identify_text_columns(df):
    """
    Identify columns in the dataframe that contain text (string-based columns).
    
    Args:
        df (pd.DataFrame): The dataframe containing the downloaded data (data.csv).
        
    Returns:
        list: A list of column names that are text-based.
    """
    text_columns = []
    for col in df.columns:
        # Check if the column contains object data type (usually string/text data)
        if df[col].dtype == 'object':
            text_columns.append(col)
    return text_columns

def main():
    csv_file = 'datasets.csv'
    webscraping_file = 'webscraping.py'
    
    # Run web scraping if the CSV file is outdated or missing
    if not os.path.isfile(csv_file) or os.path.getmtime(csv_file) < os.path.getmtime(webscraping_file):
        run_webscraping()

    # Load the dataset metadata from the CSV file
    df = pd.read_csv(csv_file)
    
    # Initialize the LLM chatbot for generating summaries
    chatbot = LLMChatbot(model_name='mistral')

    # Generate summaries using the LLM for all datasets
    print("Generating summaries using the LLM...")
    df_with_summaries = generate_summaries_for_datasets(df, chatbot)

    # Save the new summaries and links to datasets.csv
    df_with_summaries.to_csv(csv_file, index=False)
    print(f"Summaries and links saved to {csv_file}")

    # Combine the 'metadatasummary' and 'links' to create a FAISS index for metadata
    combined_text = df_with_summaries['metadatasummary'] + " " + df_with_summaries['links']

    # Create a FAISS index from the combined metadata summary and links
    print("Creating FAISS index from generated metadata summaries and links...")
    model, metadata_index = create_faiss_index(combined_text.tolist())

    # Define the user query
    query = "How many audits were planned in 2016?"

    # Use the LLM to interpret the query (classify it as 'single', 'multiple', or 'broad')
    query_type = chatbot.interpret_query(query)
    print(f"LLM classified the query as: {query_type}")

    # Adjust the number of results (k) based on the LLM's interpretation
    if query_type == 'single':
        k = 1
    elif query_type == 'multiple':
        k = 5  # Adjust as needed
    else:  # 'broad' or default case
        k = 2  # Default to 2 datasets if it's a broad or unclear request
    print(f"Determined number of results (k) for the query: {k}")

    # Perform FAISS search on metadata (first FAISS search)
    best_indices, best_distances = query_faiss_index(query, model, metadata_index, k=k)

    # Retrieve the best results based on the search and display the summaries and links
    best_metadata = df_with_summaries.iloc[best_indices]
    print(f"Best Metadata Results (Distances: {best_distances}):\n")
    print(best_metadata[['metadatasummary', 'links']])

    # Download the relevant datasets using the links from the search results
    download_datasets(df_with_summaries, best_indices, output_file='data.csv')

    # Load the downloaded datasets (data.csv)
    data_df = pd.read_csv('data.csv')

    # Identify the text columns dynamically
    text_columns = identify_text_columns(data_df)
    
    if not text_columns:
        print("No text columns found in the downloaded data.")
        return

    # Concatenate all text columns into a single string for each row
    data_df['combined_text'] = data_df[text_columns].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)

    # Convert the combined text into a list for FAISS indexing
    text_data = data_df['combined_text'].tolist()

    # Create a FAISS index for the content in data.csv (second FAISS search)
    data_model, data_index = create_faiss_index_for_data(text_data)

    # Perform a FAISS search on the data (second FAISS search)
    data_results = search_data(query, data_model, data_index, text_data)

    # Display the top data results
    for i, result in enumerate(data_results):
        print(f"Data Result {i+1}:\n{result}\n")

    # Use the LLM to generate a final answer using both FAISS search results
    final_answer = chatbot.generate_response(
        context=f"Metadata Results:\n{best_metadata[['metadatasummary', 'links']].to_string()}\n"
                f"Data Results:\n{'\n'.join(data_results)}\n\n",
        query=query  # Passing the user query here
    )

    print(f"Final Answer:\n{final_answer}")

if __name__ == "__main__":
    main()
